---
title: 对抗样本和对抗训练笔记
tags: [
    "adversarial_example",
    "machine_learning"
]
date: 2018-01-18
categories: [
    "machine_learning"
]
---

最近主要在看对抗样本，对抗训练相关的论文，在此写一下个人的一些理解，有些想法不是很成熟，欢迎交流。

## 什么是对抗样本

简单的说，就是会使得机器学习的算法产生误判的样本。比如下图![左-狗    ||    右-鸵鸟](http://upload-images.jianshu.io/upload_images/2563527-3b491283781911f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

2014年，Anh Nguyen，Jason Yosinki，Jeff Clune发表论文Deep Neural Networks are Easily Fooled，构造了一类“对抗样本”。机器视觉在这些样本上会产生戏剧性的错误。如图，深度神经网络把左图看成狗，右图看成鸵鸟。

## 为什么会产生对抗样本

1. 训练样本集不可能覆盖所有的可能性，并且很可能只能覆盖一小部分，所以不可能从中训练出一个覆盖所有样本特征的模型
2. 用模型训练分类问题的时候，目标是如何更好的分类，所以模型会尽量扩大样本和boundary之间的距离，扩大每一个class区域的空间。这样做的好处是让分类更容易，但坏处是也在每一个区域里包括了很多并不属于这个class的空间。
![image.png](http://upload-images.jianshu.io/upload_images/2563527-550a56ee37983851.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
如上图，蓝色为数据的真实决策边界（real decision boundary）,红色为模型的决策边界（model decision boundary）。可以发现，基于图中已有的数据集，红线已经是一条最后的决策边界了，但是离真实的决策边界依然有很多差别的地方，当维数增加，这样的差别会变得更加的大，也就是说会有许多的对抗样本存在。

## 模型的鲁棒性

因为对抗样本的存在，我们有必要对机器模型的评判提供一个新的评判标准，用于分析模型对于微小扰动的抵抗能力。使得模型误判需要的扰动幅度越大，那么说明模型的鲁棒性越好。

## 生成对抗样本的方法简介

在一个黑盒攻击中，我们能够得知模型对应的输入输出，由于对抗样本存在的必然性，理论上，我们只需要在原样本中随机的添加扰动，然后不断暴力尝试，测试是否攻击成功即可。

但是实际上，这样的搜索是及其耗时的，并且随着特征纬度增加，几乎是不可实现的，想要通过随机的干扰构造出对抗样本的可行性是很小的。

基于此，现在已有很多生成对抗样本相关的算法出现，下面仅做一些简单的介绍

#### Fast Gradient Sign Method（FGSM）

这个算法在Good Fellow的论文EXPLAINING AND HARNESSING ADVERSARIAL EXAMPLES中提出，主要是基于对抗样本的线性解释。之前很多人认为是由于模型非线性的特征导致了对抗样本的产生，而论文提出恰恰是模型本身的线性（或者说是通过点乘得到score的方式）引发了对抗样本。一个形象的解释如下图：

![image.png](http://upload-images.jianshu.io/upload_images/2563527-5b021a1721d6a614.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### Targeted FGSM
与FGSM主要的区别是，FGSM将样本沿着梯度下降的反方向构造样本，而targeted FGSM沿着希望模型误判的class的方向构造。

#### Iterative FGSM(I-FGSM)
上面两个FGSM的算法在构造对抗样本的时候都只进行了一部构造，也就是沿着特定的方向在一定阈值的限制下，移动一步。而I-FGSM则通过多步更小的移动，使得能够构造出更加精准的对抗样本，但同时也提升了构造的计算量，减慢了构造的速度。

#### RAND-FGSM
该算法在论文Ensemble Adversarial Training Attacks and Defenses中提出，提出的原因主要是认为：在数据点附近的损失函数会有很大的曲率，也就是不够平滑，从而导致生成的对抗样本会对自身的模型有特异性，这也解释了为什么经过对抗训练的模型对于白盒攻击的鲁棒性比黑盒攻击的鲁棒性更好这个奇怪的现象。

#### JSMA
暂时还没有进行了解
算法提出自论文The Limitations of Deep Learning in Adversarial Settings

## 对抗训练
通过在原有的模型训练过程中注入对抗样本，从而提升模型对于微小扰动的鲁棒性。如FGSM的做法就是直接修改损失函数如下：
![image.png](http://upload-images.jianshu.io/upload_images/2563527-e06f5d0cfbb553b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 黑盒攻击和白盒攻击
很容易理解，黑盒攻击就是已知输入输出的对应关系，攻击者去寻找对抗样本来实现对模型的攻击。而白盒攻击就是已知模型的所有结构和知识，来实现对模型的攻击。在论文Practical Black-Box Attacks against Machine Learning中，提出对于模型的黑盒攻击可以通过观察其输入输出的对应关系，构造一个相似的机器学习模型，然后对其进行白盒攻击，得到的对抗样本通常也具有迁移性，能够对需要攻击的黑盒达到很高的成功率。

## 总结
对抗样本对抗训练这一块依然还有许多要研究的地方，有许多地方依然还没有得到很好的结论。比如，

1. 对抗样本的迁移性产生的原因到底是什么？为什么使用FGSM等一步的算法生成的对抗样本有很优秀的迁移性，而使用多步迭代的算法却无法很好的迁移？

2. 如何去正确评判一个模型针对扰动的抵抗性？在论文Ensemble Adversarial Training Attacks and Defenses做了一下阐述，提到了包括同时进行黑盒和白盒攻击检测的重要性。不同的对抗样本生成算法会有不同的特性，因此，模型的不同的生成算法的抵抗性也不同。
